{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMmHgbSYYqyA2f8mSm5eayU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c29077590907400daafa7e99c52a38ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b5d06e0bf2634c70ae5a49192cbd0bb6",
              "IPY_MODEL_05ac7f8c525b4d1ab5eb7ddb1e33175a",
              "IPY_MODEL_4cebae618a9b4c968410ff9a2607a0e5"
            ],
            "layout": "IPY_MODEL_53f4b51299e1469a89426d09ef02fb69"
          }
        },
        "b5d06e0bf2634c70ae5a49192cbd0bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0762a327018493aaa68f9ddc239c47b",
            "placeholder": "​",
            "style": "IPY_MODEL_517403edbdcf4a6294b31baa16bd3db1",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "05ac7f8c525b4d1ab5eb7ddb1e33175a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d91749a42b7407cac582c08f733339c",
            "max": 39,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c300a9a7cec47f38a52e3ded96cacb6",
            "value": 39
          }
        },
        "4cebae618a9b4c968410ff9a2607a0e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62c2455848af431aa08caa110c8579a4",
            "placeholder": "​",
            "style": "IPY_MODEL_f3070669313b499f9c221cc4e4dc16d8",
            "value": " 39.0/39.0 [00:00&lt;00:00, 1.55kB/s]"
          }
        },
        "53f4b51299e1469a89426d09ef02fb69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0762a327018493aaa68f9ddc239c47b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "517403edbdcf4a6294b31baa16bd3db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8d91749a42b7407cac582c08f733339c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c300a9a7cec47f38a52e3ded96cacb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62c2455848af431aa08caa110c8579a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3070669313b499f9c221cc4e4dc16d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62f67479e53d4fc1bb4c9e6f377dde09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae80b051f6444b6794ab572e9d68b465",
              "IPY_MODEL_c8c0bf65fd9a4aa08ba08ba4521cf57d",
              "IPY_MODEL_f50a9ea8b3ba4f96bc2b7de0137a29cb"
            ],
            "layout": "IPY_MODEL_71633b52de6c4491abcb07aaeac17479"
          }
        },
        "ae80b051f6444b6794ab572e9d68b465": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cc301a358394d6ca0f5668dfb42ffd3",
            "placeholder": "​",
            "style": "IPY_MODEL_30a6c420c8d3400d9916650139b0706e",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "c8c0bf65fd9a4aa08ba08ba4521cf57d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1f23b7c5f0a4fc6b2a2613c0effc802",
            "max": 465,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_319484f00a87490583b8bc8e06ffde35",
            "value": 465
          }
        },
        "f50a9ea8b3ba4f96bc2b7de0137a29cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec616b89f5064d8a8c8a862270c996d8",
            "placeholder": "​",
            "style": "IPY_MODEL_6a3cb0de1e074ab5807d7f1f8c0c0abb",
            "value": " 465/465 [00:00&lt;00:00, 27.2kB/s]"
          }
        },
        "71633b52de6c4491abcb07aaeac17479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cc301a358394d6ca0f5668dfb42ffd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a6c420c8d3400d9916650139b0706e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1f23b7c5f0a4fc6b2a2613c0effc802": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "319484f00a87490583b8bc8e06ffde35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ec616b89f5064d8a8c8a862270c996d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3cb0de1e074ab5807d7f1f8c0c0abb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed23b32b99ab44fabdfdc34f229d14fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_246f4bce04f44bae81bf4d63438ab021",
              "IPY_MODEL_04ca562eae8045288857406289b1fc1a",
              "IPY_MODEL_1caab333aa83498eb26a99646a25602b"
            ],
            "layout": "IPY_MODEL_040ee18e9eb949e28b0ac9295323c66d"
          }
        },
        "246f4bce04f44bae81bf4d63438ab021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03a5ec2e6f414bd0893b80860dc858de",
            "placeholder": "​",
            "style": "IPY_MODEL_c61639d1f37046e0beb38d4e0d46bacc",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "04ca562eae8045288857406289b1fc1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b65b6f68bee452bbbb8d0c84e62f01f",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85e9f5f2aa9e46df9c31e248ab200e92",
            "value": 231508
          }
        },
        "1caab333aa83498eb26a99646a25602b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_620e06f7a4994c9194ab7385476da6a1",
            "placeholder": "​",
            "style": "IPY_MODEL_c4734a9587724a4ca4871c5f03197ab2",
            "value": " 232k/232k [00:00&lt;00:00, 3.58MB/s]"
          }
        },
        "040ee18e9eb949e28b0ac9295323c66d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03a5ec2e6f414bd0893b80860dc858de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c61639d1f37046e0beb38d4e0d46bacc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b65b6f68bee452bbbb8d0c84e62f01f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e9f5f2aa9e46df9c31e248ab200e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "620e06f7a4994c9194ab7385476da6a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4734a9587724a4ca4871c5f03197ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "627e99ccc38841809a308c3e679ff744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52a143192d024eae9361d3da4bb84db3",
              "IPY_MODEL_64fd3e64333744c3abc3a561c711c2a2",
              "IPY_MODEL_7cf9922c4c01478d926bff701510032f"
            ],
            "layout": "IPY_MODEL_3e9917018e334db7b511785fad8f34c3"
          }
        },
        "52a143192d024eae9361d3da4bb84db3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd702ed88b114ebaa1e781f135f17cd7",
            "placeholder": "​",
            "style": "IPY_MODEL_eb6e42f65e854471aa0beb1a986d5b42",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "64fd3e64333744c3abc3a561c711c2a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_311419ed48964494ab802784219e0433",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15c2590669be4e9e89020eb0a4807a3e",
            "value": 112
          }
        },
        "7cf9922c4c01478d926bff701510032f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41161df990374bba9497f92a3007e898",
            "placeholder": "​",
            "style": "IPY_MODEL_ed0785b4934f4c8fb7de3e215965a361",
            "value": " 112/112 [00:00&lt;00:00, 5.20kB/s]"
          }
        },
        "3e9917018e334db7b511785fad8f34c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd702ed88b114ebaa1e781f135f17cd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb6e42f65e854471aa0beb1a986d5b42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "311419ed48964494ab802784219e0433": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c2590669be4e9e89020eb0a4807a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41161df990374bba9497f92a3007e898": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed0785b4934f4c8fb7de3e215965a361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a05384b746548d58fe192d10c2a9417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73e07c0397514adf8dfefb4c14da2b51",
              "IPY_MODEL_828dbcd064e4417aa8bde1a46fbfb19e",
              "IPY_MODEL_408e21e4cf804bbaaf9c1c3f3e50b6ed"
            ],
            "layout": "IPY_MODEL_5faa7858745e46f48ad8a8538422df22"
          }
        },
        "73e07c0397514adf8dfefb4c14da2b51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_809487cb9abb491cbb95ee01e99868fb",
            "placeholder": "​",
            "style": "IPY_MODEL_83e47b1827cf49e298437d64a89c3872",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "828dbcd064e4417aa8bde1a46fbfb19e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc7f2cf94ca044bb8d464aa493824554",
            "max": 437985356,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f05d00c6ca6748e4b310efb47573314b",
            "value": 437985356
          }
        },
        "408e21e4cf804bbaaf9c1c3f3e50b6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23bfe01866c9457e9e27fe1a42977087",
            "placeholder": "​",
            "style": "IPY_MODEL_71f8615ceb0b4582b2fd7420eee91ca2",
            "value": " 438M/438M [00:05&lt;00:00, 113MB/s]"
          }
        },
        "5faa7858745e46f48ad8a8538422df22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "809487cb9abb491cbb95ee01e99868fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83e47b1827cf49e298437d64a89c3872": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc7f2cf94ca044bb8d464aa493824554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f05d00c6ca6748e4b310efb47573314b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23bfe01866c9457e9e27fe1a42977087": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71f8615ceb0b4582b2fd7420eee91ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Spartan-119/A-B-Testing-Approach-for-Comparing-Performance-of-ML-Models/blob/main/a_b_testing_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSkHP5LdeEgj",
        "outputId": "60c0f766-2377-4215-91c5-d5a2c5e0e47e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.15.2+cu118)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.0.2+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.12.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.57.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.4.4)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.41.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# installing all the necessary packages\n",
        "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116 --upgrade\n",
        "!pip install transformers --upgrade\n",
        "!pip install tqdm\n",
        "!pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all the necessary libraries\n",
        "import json\n",
        "import os\n",
        "import timeit\n",
        "import collections\n",
        "import time\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, squad_convert_examples_to_features\n",
        "from transformers.data.processors.squad import SquadV2Processor,SquadResult\n",
        "from transformers.data.metrics.squad_metrics import (\n",
        "    compute_predictions_log_probs,\n",
        "    compute_predictions_logits,\n",
        "    squad_evaluate,\n",
        ")"
      ],
      "metadata": {
        "id": "WmYit7ejepV4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DO_LOWER_CASE = True\n",
        "NBEST_SIZE = 20\n",
        "DOC_STRIDE = 128\n",
        "MAX_SEQ_LENGTH = 384\n",
        "MAX_QUERY_LENGTH = 64\n",
        "MAX_ANSWER_LENGTH = 30\n",
        "DATA_DIR = 'data/squad'\n",
        "PREDICT_FILE = 'dev-v2.0.json'\n",
        "\n",
        "BERT_MODEL_TYPE = 'bert'\n",
        "BERT_MODEL_HF_PATH = 'twmkn9/bert-base-uncased-squad2'\n",
        "BERT_OUTPUT_DIR = 'models/bert/twmkn9_bert-case-uncased-squad2'\n",
        "\n",
        "DISTILBERT_MODEL_TYPE = 'distilbert'\n",
        "DISTILBERT_MODEL_HF_PATH = 'twmkn9/distilbert-base-uncased-squad2'\n",
        "DISTILBERT_OUTPUT_DIR = 'models/distilbert/twmkn9_distilbert-base-uncased-squad2'"
      ],
      "metadata": {
        "id": "A5iBdFlYf4cp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and Exploring the dataset"
      ],
      "metadata": {
        "id": "GIdkYBU4qUxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading the dataset\n",
        "!wget -P data/squad/ https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAeVniE3r3FY",
        "outputId": "3f17b101-f20d-4094-c628-37d658a52c5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-01 08:08:52--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n",
            "Resolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.108.153, 185.199.109.153, 185.199.110.153, ...\n",
            "Connecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.108.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4370528 (4.2M) [application/json]\n",
            "Saving to: ‘data/squad/dev-v2.0.json’\n",
            "\n",
            "\rdev-v2.0.json         0%[                    ]       0  --.-KB/s               \rdev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-09-01 08:08:53 (76.2 MB/s) - ‘data/squad/dev-v2.0.json’ saved [4370528/4370528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <i>Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.\n",
        "\n",
        "#### SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.</i>\n"
      ],
      "metadata": {
        "id": "4kyRIb-GsgO7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the DEV set using Hugging Face's data processors"
      ],
      "metadata": {
        "id": "ydfV2-ZbuFbs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am going to make use of [Processors](https://huggingface.co/transformers/main_classes/processors.html) to facilitate basic processing tasks with some canonical NLP datasets. The processors can be used for loading datasets and converting their examples to features for direct use in the model. More specifically, we will be using the [SQuAD processors](https://huggingface.co/transformers/main_classes/processors.html#squad)"
      ],
      "metadata": {
        "id": "zvhSzVUludK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_list(tensor):\n",
        "  return tensor.detach().cpu().tolist()"
      ],
      "metadata": {
        "id": "zbrTlTo5r91S"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_cache_examples(model_name_or_path,\n",
        "                            data_dir= DATA_DIR,\n",
        "                            predict_file=PREDICT_FILE,\n",
        "                            max_seq_length=MAX_SEQ_LENGTH,\n",
        "                            doc_stride=DOC_STRIDE,\n",
        "                            max_query_length=MAX_QUERY_LENGTH,\n",
        "                            overwrite_cache=True):\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
        "    # Load data features from cache or dataset file\n",
        "    input_dir = data_dir if data_dir else \".\"\n",
        "    cached_features_file = os.path.join(\n",
        "        input_dir,\n",
        "        \"cached_{}_{}_{}\".format(\n",
        "            \"dev\",\n",
        "            list(filter(None, model_name_or_path.split(\"/\"))).pop(),\n",
        "            str(max_seq_length),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Init features and dataset from cache if it exists\n",
        "    if os.path.exists(cached_features_file) and not overwrite_cache:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features_and_dataset = torch.load(cached_features_file)\n",
        "        features, dataset, examples = (\n",
        "            features_and_dataset[\"features\"],\n",
        "            features_and_dataset[\"dataset\"],\n",
        "            features_and_dataset[\"examples\"],\n",
        "        )\n",
        "    else:\n",
        "\n",
        "        processor = SquadV2Processor()\n",
        "\n",
        "        examples = processor.get_dev_examples(data_dir, filename=predict_file)\n",
        "\n",
        "        features, dataset = squad_convert_examples_to_features(\n",
        "            examples=examples,\n",
        "            tokenizer=tokenizer,\n",
        "            max_seq_length=max_seq_length,\n",
        "            doc_stride=doc_stride,\n",
        "            max_query_length=max_query_length,\n",
        "            is_training=False,\n",
        "            return_dataset=\"pt\",\n",
        "            threads=1,\n",
        "        )\n",
        "\n",
        "\n",
        "    return dataset, examples, features"
      ],
      "metadata": {
        "id": "sDvb8A8qut0m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, examples, features = load_and_cache_examples(BERT_MODEL_HF_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197,
          "referenced_widgets": [
            "c29077590907400daafa7e99c52a38ee",
            "b5d06e0bf2634c70ae5a49192cbd0bb6",
            "05ac7f8c525b4d1ab5eb7ddb1e33175a",
            "4cebae618a9b4c968410ff9a2607a0e5",
            "53f4b51299e1469a89426d09ef02fb69",
            "f0762a327018493aaa68f9ddc239c47b",
            "517403edbdcf4a6294b31baa16bd3db1",
            "8d91749a42b7407cac582c08f733339c",
            "3c300a9a7cec47f38a52e3ded96cacb6",
            "62c2455848af431aa08caa110c8579a4",
            "f3070669313b499f9c221cc4e4dc16d8",
            "62f67479e53d4fc1bb4c9e6f377dde09",
            "ae80b051f6444b6794ab572e9d68b465",
            "c8c0bf65fd9a4aa08ba08ba4521cf57d",
            "f50a9ea8b3ba4f96bc2b7de0137a29cb",
            "71633b52de6c4491abcb07aaeac17479",
            "2cc301a358394d6ca0f5668dfb42ffd3",
            "30a6c420c8d3400d9916650139b0706e",
            "e1f23b7c5f0a4fc6b2a2613c0effc802",
            "319484f00a87490583b8bc8e06ffde35",
            "ec616b89f5064d8a8c8a862270c996d8",
            "6a3cb0de1e074ab5807d7f1f8c0c0abb",
            "ed23b32b99ab44fabdfdc34f229d14fa",
            "246f4bce04f44bae81bf4d63438ab021",
            "04ca562eae8045288857406289b1fc1a",
            "1caab333aa83498eb26a99646a25602b",
            "040ee18e9eb949e28b0ac9295323c66d",
            "03a5ec2e6f414bd0893b80860dc858de",
            "c61639d1f37046e0beb38d4e0d46bacc",
            "3b65b6f68bee452bbbb8d0c84e62f01f",
            "85e9f5f2aa9e46df9c31e248ab200e92",
            "620e06f7a4994c9194ab7385476da6a1",
            "c4734a9587724a4ca4871c5f03197ab2",
            "627e99ccc38841809a308c3e679ff744",
            "52a143192d024eae9361d3da4bb84db3",
            "64fd3e64333744c3abc3a561c711c2a2",
            "7cf9922c4c01478d926bff701510032f",
            "3e9917018e334db7b511785fad8f34c3",
            "dd702ed88b114ebaa1e781f135f17cd7",
            "eb6e42f65e854471aa0beb1a986d5b42",
            "311419ed48964494ab802784219e0433",
            "15c2590669be4e9e89020eb0a4807a3e",
            "41161df990374bba9497f92a3007e898",
            "ed0785b4934f4c8fb7de3e215965a361"
          ]
        },
        "id": "zEPdgAniyH4Y",
        "outputId": "7dfbeca3-a8b2-4a26-a224-fcded5263081"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c29077590907400daafa7e99c52a38ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62f67479e53d4fc1bb4c9e6f377dde09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed23b32b99ab44fabdfdc34f229d14fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "627e99ccc38841809a308c3e679ff744"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:12<00:00,  2.71it/s]\n",
            "convert squad examples to features: 100%|██████████| 11873/11873 [02:42<00:00, 73.19it/s]\n",
            "add example index and unique id: 100%|██████████| 11873/11873 [00:00<00:00, 593700.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'There are {len(examples)} examples in the dev dataset.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X17jPP7tyQRm",
        "outputId": "a9e085ca-d63a-48a8-a82f-705e41671daf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 11873 examples in the dev dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This list of examples contains objects of type transformers.data.processors.squad.SquadExample.\n",
        "We use the functionbelow to extract the information we want from such objects.\n",
        "More specifically: 'qid', 'question_text', 'context_text' and 'answer'."
      ],
      "metadata": {
        "id": "iuXX6LRwk2pq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will first create some extra variables to help on manipulation of data."
      ],
      "metadata": {
        "id": "g7Y8-ozplQ35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating some maps to help identify examples of interest.\n",
        "qid_to_example_index = {example.qas_id: i for i, example in enumerate(examples)}\n",
        "qid_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}\n",
        "answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if has_answer]\n",
        "no_answer_qids = [qas_id for qas_id, has_answer in qid_to_has_answer.items() if not has_answer]"
      ],
      "metadata": {
        "id": "MpRFtUtClPVN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And also, the function below to help on extracting information given a `qid` (question unique identifier)"
      ],
      "metadata": {
        "id": "h9cI61WBmK65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_example(qid: str) -> None:\n",
        "  idx = qid_to_example_index[qid]\n",
        "  q = examples[idx].question_text\n",
        "  c = examples[idx].context_text\n",
        "  a = [answer['text'] for answer in examples[idx].answers]\n",
        "\n",
        "  print(f'Examples {idx} of {len(examples)}\\n------------------')\n",
        "  print(f'Q: {q}\\n')\n",
        "  print('Context:')\n",
        "  pprint(c)\n",
        "  print(f'\\nTrue Answers:\\n{a}')"
      ],
      "metadata": {
        "id": "HUKy5JGFmCwK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Positive Example"
      ],
      "metadata": {
        "id": "BzU2wpTunWgP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "50% of the examples in the test set are questionst hat have answers contained within their corresponding passage. In these cases, up to 5 possible correct answers are provided. Such answers must come directly from the passage, we will see later, however, that there are several ways to arrive at a \"correct\" answer."
      ],
      "metadata": {
        "id": "hDQ9-8LOnZRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_example(answer_qids[2456])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5MtCsUxnVi1",
        "outputId": "062cce3e-9a67-45d9-fdd5-adbc0c48db52"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples 4959 of 11873\n",
            "------------------\n",
            "Q: It is now possible to convert old relative ages into what type of ages using isotopic dating?\n",
            "\n",
            "Context:\n",
            "('At the beginning of the 20th century, important advancement in geological '\n",
            " 'science was facilitated by the ability to obtain accurate absolute dates to '\n",
            " 'geologic events using radioactive isotopes and other methods. This changed '\n",
            " 'the understanding of geologic time. Previously, geologists could only use '\n",
            " 'fossils and stratigraphic correlation to date sections of rock relative to '\n",
            " 'one another. With isotopic dates it became possible to assign absolute ages '\n",
            " 'to rock units, and these absolute dates could be applied to fossil sequences '\n",
            " 'in which there was datable material, converting the old relative ages into '\n",
            " 'new absolute ages.')\n",
            "\n",
            "True Answers:\n",
            "['absolute ages', 'rock units', 'new absolute']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative Example"
      ],
      "metadata": {
        "id": "mo7vdGEjn7F6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The remaining 50% of the questions in the test set do not have an answer. This is important as in a real life Q&A system, the model needs to learn when **NOT TO ANSWER.**"
      ],
      "metadata": {
        "id": "BtL6WUT5n9Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_example(no_answer_qids[1235])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAa9F5kGnv2P",
        "outputId": "aaa03cd2-f232-45ca-eca8-c8da06c070be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Examples 2520 of 11873\n",
            "------------------\n",
            "Q: What is difficult with a satellite-to-noise ratio?\n",
            "\n",
            "Context:\n",
            "('Oxygen presents two spectrophotometric absorption bands peaking at the '\n",
            " 'wavelengths 687 and 760 nm. Some remote sensing scientists have proposed '\n",
            " 'using the measurement of the radiance coming from vegetation canopies in '\n",
            " 'those bands to characterize plant health status from a satellite platform. '\n",
            " 'This approach exploits the fact that in those bands it is possible to '\n",
            " \"discriminate the vegetation's reflectance from its fluorescence, which is \"\n",
            " 'much weaker. The measurement is technically difficult owing to the low '\n",
            " 'signal-to-noise ratio and the physical structure of vegetation; but it has '\n",
            " 'been proposed as a possible method of monitoring the carbon cycle from '\n",
            " 'satellites on a global scale.')\n",
            "\n",
            "True Answers:\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics for Q&A Systems"
      ],
      "metadata": {
        "id": "wOWgT23212sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When measuring the performance of a machine learning system, we need to think about both **model** and **customer metrics.**"
      ],
      "metadata": {
        "id": "z0sxc5GV3OZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q&A systems are usually measured by two dominant metrics: **F1** and **Exact Match (EM)**. They are computed on individual question and answer **pairs**. When multiple correct answers are available for a given question the maximum score over all possible correct answers is computed. Overall EM and F1 scores are computed for a model by averaging over the individual example scores."
      ],
      "metadata": {
        "id": "MQfRt0bl3jYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exact Match:"
      ],
      "metadata": {
        "id": "r_VGFueB4Re3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each Q&A pair, if the characters of the model's prediction are an exact matvh of the characters of any of the True Answer(s), **EM = 1**, else, **EM = 0**. This is a strict all-or-nothing metric, which may have little value for final customers of a Q&A system. It may be beneficial only when assessing against a negative example; if the model predicts any text at all, it automatically receives a 0 for that example."
      ],
      "metadata": {
        "id": "rhsLDwIq4cIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F1 Score:"
      ],
      "metadata": {
        "id": "_Q_76Kdg3iVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all classification problems rely on F1 score to measure the model performance. It is mostly appropriate when we care equally about precision and recall. On an QnA system, however, it is computed over the individual words in the prediction against those in the True answer. The number of shared words between the prediction and the trust is the basis of F1 score. While **precision** is the ratio between the number of shared words to the total number of words in the prediction; **Recall** is the ratio of the number of shared words to the toal number of words in the ground truth."
      ],
      "metadata": {
        "id": "kHC_C8iP5Fka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latency"
      ],
      "metadata": {
        "id": "NE0ffj168oi3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latency is an important metric for ML systems. in the QnA example, it is of the utmost importance when the system is used in a conversational application. <br> For Example, Alexa and Google home are devices that have very strict latency constraints as the users expect an answer within a few seconds or macroseconds after the question is asked. When updating models we should take this dimension according to the application of the system."
      ],
      "metadata": {
        "id": "rC-trkOp8p--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer Rate"
      ],
      "metadata": {
        "id": "LDGsFCD49ORY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the QnA systems, models that attempt to answer every question are often perceived inaccurate. The system should only provide an output when it is confident enough to do so. In other words, when the probabiliries of predictions are above a certain threshold. <br> In some applications, a model should be able to say *I don't know* or *The context has not enough information to answer the question.*"
      ],
      "metadata": {
        "id": "5H_rtE4z9SUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q&A Models"
      ],
      "metadata": {
        "id": "hX-3eJSPIIe4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question and Answering make use of Large Language Models (LLMs) as any other classification problem in NLP. The main difference relies on how the input and the output is provided to the model. Generally speaking models are trained to match the true answer to the question as they are provided together as an input to the model."
      ],
      "metadata": {
        "id": "MDHD3B31oTuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert"
      ],
      "metadata": {
        "id": "OgMAI1ClI4NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT is a neural approach to pre-train language representations which obtains near SOTA results on a wide array of NLP tasks, including SQuAD Question Answering dataset <br><br>\n",
        "\n",
        "Developed in 2019, BERT achieves 80.422% in the EM score and 83.118% in the F1 score.<br><br>\n",
        "\n",
        "BERT-base has 110 million parameters and BERT-large has 340 million parameters."
      ],
      "metadata": {
        "id": "iHUtuP7oI6Y8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Parameters comparison"
      ],
      "metadata": {
        "id": "sAqAtSqAJgfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As LLMs were developed, the amount of parameters in these models have grown exponentially. Although this improves model performance, it comes at a cost: Latency. As it will be discussed, for use cases where inference is done on batches that may have less impact, however, on real time systems such as voice assistants or web search, latency plays a major role on deciding whether one model is better than the other."
      ],
      "metadata": {
        "id": "Z9U-RViiJkYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT Input"
      ],
      "metadata": {
        "id": "6oXV30yJKP6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[CLS] context [SEP] question [SEP] [PAD] [PAD] [PAD]"
      ],
      "metadata": {
        "id": "kpCoPdGMKSKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**context** = \"The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body under the auspices of the United Nations.\"\n",
        "\n",
        "**question** = \"What organization is the IPCC a part of?\"\n",
        "\n",
        "**after being merged by the tokenizer**:\n",
        "```\n",
        "\"[CLS] The Intergovernmental Panel on Climate Change (IPCC) is a scientific intergovernmental body under the auspices of the United Nations. [SEP] What organization is the IPCC a part of? [SEP] [PAD] [PAD] [PAD]\"\n",
        "```\n"
      ],
      "metadata": {
        "id": "cemFTJ-dKVq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**token-id format**:\n",
        "\n",
        "[101, 1109, 11300, 2758, 24472, 15595, 20339, 1113, 13540, 9091, 113, 14274, 12096, 114, 1110, 170, 3812,\n",
        " 9455, 2758, 24472, 15595, 1404, 1223, 1103, 22105, 1104, 1103, 1244, 3854, 119, 102, 1327, 2369, 1110, 1103,\n",
        " 14274, 12096, 170, 1226, 1104, 136, 102, 0, 0, 0]"
      ],
      "metadata": {
        "id": "iYgjajNpKcsZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Pre-Trained BERT from Hugging Face's repository."
      ],
      "metadata": {
        "id": "3M-0PYH3Kelv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_HF_PATH, use_fast = False)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(BERT_MODEL_HF_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "9a05384b746548d58fe192d10c2a9417",
            "73e07c0397514adf8dfefb4c14da2b51",
            "828dbcd064e4417aa8bde1a46fbfb19e",
            "408e21e4cf804bbaaf9c1c3f3e50b6ed",
            "5faa7858745e46f48ad8a8538422df22",
            "809487cb9abb491cbb95ee01e99868fb",
            "83e47b1827cf49e298437d64a89c3872",
            "dc7f2cf94ca044bb8d464aa493824554",
            "f05d00c6ca6748e4b310efb47573314b",
            "23bfe01866c9457e9e27fe1a42977087",
            "71f8615ceb0b4582b2fd7420eee91ca2"
          ]
        },
        "id": "aVRdXn2pKYtm",
        "outputId": "dd4350f6-51ca-49a2-d7cb-3d376ab355a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a05384b746548d58fe192d10c2a9417"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at twmkn9/bert-base-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utility Functions"
      ],
      "metadata": {
        "id": "I4KWRKCbK1Hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a question_id, model and tokenizer, we get an answer text. In here we get the maximum probability of beginning and end for the answer in the softmax output."
      ],
      "metadata": {
        "id": "oXWdDyL-K-it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(qid: str, model: AutoModelForQuestionAnswering, tokenizer: AutoTokenizer):\n",
        "  # given a question_id (qas_id or qid), load the example, get the model outputs and generate an answer\n",
        "  question = examples[qid_to_example_index[qid]].question_text\n",
        "  context = examples[qid_to_example_index[qid]].context_text\n",
        "\n",
        "  inputs = tokenizer.encode_plus(question, context, return_tensors = 'pt')\n",
        "\n",
        "  outputs = model(**inputs)\n",
        "\n",
        "  answer_start = torch.argmax(outputs[0])       # get the most likely beginning of answer with the argmax of the score\n",
        "  answer_end = torch.argmax(outputs[1]) + 1\n",
        "\n",
        "  answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start: answer_end]))\n",
        "\n",
        "  return answer"
      ],
      "metadata": {
        "id": "7iHkRN1lLKtt"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am creatinga simple function that given an example list, it extracts the gold answers."
      ],
      "metadata": {
        "id": "f1T3kGjsMQib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gold_answers(example):\n",
        "  \"\"\"\n",
        "  helper function that retrieves all possible true answers from a squad2.0 example.\n",
        "  \"\"\"\n",
        "\n",
        "  gold_answers = [answer['text'] for answer in example.answers if answer['text']]\n",
        "\n",
        "  # if gold_answers doesn't exist, it's because this is a negative example.\n",
        "  # the only correct answer is an empty string then in this case.\n",
        "  if not gold_answers:\n",
        "    gold_answers = [\"\"]\n",
        "\n",
        "  return gold_answers"
      ],
      "metadata": {
        "id": "3pBy7UtBMOVq"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For metrics like EM, we need to make sure that texts are normalized so we can compare on a character level."
      ],
      "metadata": {
        "id": "QsrAMtQjM1V5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# these functions are heavily influenced by the HF squad_metrics.py script\n",
        "def normalize_text(s: str):\n",
        "  \"\"\"\n",
        "  removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\n",
        "  \"\"\"\n",
        "  import string, re\n",
        "\n",
        "  def remove_articles(text):\n",
        "    regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "    return re.sub(regex, \" \", text)\n",
        "\n",
        "  def white_space_fix(text):\n",
        "    return \" \".join(text.split())\n",
        "\n",
        "  def remove_punc(text):\n",
        "    exclude = set(string.punctuation)\n",
        "    return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "  def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "  return white_space_fix(remove_articles(remove_punc(lower(s))))"
      ],
      "metadata": {
        "id": "orPI3BDpM9li"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics Calculation"
      ],
      "metadata": {
        "id": "uQpuQGQ0N4lb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exact Match (EM)"
      ],
      "metadata": {
        "id": "P4AsIkLtbp0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_exact_match(prediction, truth):\n",
        "  return int(normalize_text(prediction) == normalize_text(truth))"
      ],
      "metadata": {
        "id": "ad3aEmhbbsQ6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F1 Score"
      ],
      "metadata": {
        "id": "eiuMMikTb-h_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_f1(prediction, truth):\n",
        "  pred_tokens = normalize_text(prediction).split()\n",
        "  truth_tokens = normalize_text(truth).split()\n",
        "\n",
        "  # if either the prediction or the truth is no-answer then f1 = 1 if they agree, else 0\n",
        "  if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
        "    return int(pred_tokens == truth_tokens)\n",
        "\n",
        "  common_tokens = set(pred_tokens) & set(truth_tokens)\n",
        "\n",
        "  # if there are no common tokens then f1 = 0\n",
        "  if len(common_tokens) ==  0:\n",
        "    return 0\n",
        "\n",
        "  prec = len(common_tokens) / len(pred_tokens)\n",
        "  rec = len(common_tokens) / len(truth_tokens)\n",
        "\n",
        "  return 2 * (prec * rec) / (prec + rec)"
      ],
      "metadata": {
        "id": "aj5W32hscBFK"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing EM and F1 for an example with a gold answer"
      ],
      "metadata": {
        "id": "bWX4AbVoc8tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = get_prediction(answer_qids[1303], model, tokenizer, )\n",
        "example = examples[qid_to_example_index[answer_qids[1303]]]\n",
        "\n",
        "gold_answers = get_gold_answers(example)\n",
        "\n",
        "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "\n",
        "print(f\"Question: {example.question_text}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"True Answers: {gold_answers}\")\n",
        "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2D7wSwHdApc",
        "outputId": "a3a0d321-3129-4078-91fb-1a4deb5d0175"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What measurement do scientists used to determine the quality of water?\n",
            "Prediction: biochemical oxygen demand\n",
            "True Answers: ['biochemical oxygen demand', 'biochemical oxygen demand', \"measuring the water's biochemical oxygen demand\", 'biochemical oxygen demand', \"measuring the water's biochemical oxygen demand\"]\n",
            "EM: 1 \t F1: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's try and compute an example without answer"
      ],
      "metadata": {
        "id": "heSct5gRePlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = get_prediction(no_answer_qids[1254], model, tokenizer)\n",
        "example = examples[qid_to_example_index[no_answer_qids[1254]]]\n",
        "\n",
        "gold_answers = get_gold_answers(example)\n",
        "\n",
        "em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "\n",
        "print(f\"Question: {example.question_text}\")\n",
        "print(f\"Prediction: {prediction}\")\n",
        "print(f\"True Answers: {gold_answers}\")\n",
        "print(f\"EM: {em_score} \\t F1: {f1_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCMRvtp0gHAb",
        "outputId": "1adb778e-dc2c-4d22-a5ca-ca7640455088"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What happened 3.7-2 billion years ago?\n",
            "Prediction: [CLS]\n",
            "True Answers: ['']\n",
            "EM: 0 \t F1: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both metrics are zero, this model does not correctly assess that this question is unanswerable. It predicts the [CLS] token (it means it considers the entire context as an answer to the question)"
      ],
      "metadata": {
        "id": "8ZCO1WFZgJqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting it all together"
      ],
      "metadata": {
        "id": "G_Q0VhVBgcYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answers_metrics(\n",
        "    model: AutoModelForQuestionAnswering,\n",
        "    tokenizer: AutoTokenizer,\n",
        "    answer_qids = answer_qids,\n",
        "    examples = examples):\n",
        "\n",
        "  answers_arr = []\n",
        "  start_time = time.time()\n",
        "  errors = []\n",
        "\n",
        "  for qid in tqdm(answer_qids):\n",
        "    try:\n",
        "      prediction = get_prediction(qid, model, tokenizer)\n",
        "      example = examples[qid_to_example_index[qid]]\n",
        "\n",
        "      gold_answers = get_gold_answers(example)\n",
        "\n",
        "      em_score = max((compute_exact_match(prediction, answer)) for answer in gold_answers)\n",
        "      f1_score = max((compute_f1(prediction, answer)) for answer in gold_answers)\n",
        "\n",
        "      result_dict = {}\n",
        "      result_dict[\"qid\"] = qid\n",
        "      result_dict[\"question\"] = example.question_text\n",
        "      result_dict[\"prediction\"] = prediction\n",
        "      result_dict[\"true_answers\"] = ';'.join(gold_answers)\n",
        "      result_dict[\"f1\"] = f1_score\n",
        "      result_dict[\"em\"] = em_score\n",
        "      answers_arr.append(result_dict)\n",
        "    except:\n",
        "      errors.append(qid)\n",
        "\n",
        "  end_time = time.time()\n",
        "\n",
        "  return pd.DataFrame(answers_arr), end_time - start_time, errors"
      ],
      "metadata": {
        "id": "w7tuTfofggDW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df, total_time, errors = get_answers_metrics(model, tokenizer, answer_qids[: 100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1ufql52g0Mt",
        "outputId": "a44bd9ef-09af-4b71-f625-913a2baa4a59"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:02<00:00,  1.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5g-wngj5iTV3",
        "outputId": "9106ac60-0082-427b-852b-a7f78440e88a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        qid  \\\n",
              "0  56ddde6b9a695914005b9628   \n",
              "1  56ddde6b9a695914005b9629   \n",
              "2  56ddde6b9a695914005b962a   \n",
              "3  56ddde6b9a695914005b962b   \n",
              "4  56ddde6b9a695914005b962c   \n",
              "\n",
              "                                            question  \\\n",
              "0               In what country is Normandy located?   \n",
              "1                 When were the Normans in Normandy?   \n",
              "2      From which countries did the Norse originate?   \n",
              "3                          Who was the Norse leader?   \n",
              "4  What century did the Normans first gain their ...   \n",
              "\n",
              "                     prediction  \\\n",
              "0                        france   \n",
              "1       10th and 11th centuries   \n",
              "2  denmark , iceland and norway   \n",
              "3                         rollo   \n",
              "4                          10th   \n",
              "\n",
              "                                        true_answers   f1  em  \n",
              "0                        France;France;France;France  1.0   1  \n",
              "1  10th and 11th centuries;in the 10th and 11th c...  1.0   1  \n",
              "2  Denmark, Iceland and Norway;Denmark, Iceland a...  1.0   1  \n",
              "3                            Rollo;Rollo;Rollo;Rollo  1.0   1  \n",
              "4  10th century;the first half of the 10th centur...  1.0   1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b5c1a208-4dcb-4a92-96ce-d0ac89a716af\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question</th>\n",
              "      <th>prediction</th>\n",
              "      <th>true_answers</th>\n",
              "      <th>f1</th>\n",
              "      <th>em</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>56ddde6b9a695914005b9628</td>\n",
              "      <td>In what country is Normandy located?</td>\n",
              "      <td>france</td>\n",
              "      <td>France;France;France;France</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56ddde6b9a695914005b9629</td>\n",
              "      <td>When were the Normans in Normandy?</td>\n",
              "      <td>10th and 11th centuries</td>\n",
              "      <td>10th and 11th centuries;in the 10th and 11th c...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>56ddde6b9a695914005b962a</td>\n",
              "      <td>From which countries did the Norse originate?</td>\n",
              "      <td>denmark , iceland and norway</td>\n",
              "      <td>Denmark, Iceland and Norway;Denmark, Iceland a...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>56ddde6b9a695914005b962b</td>\n",
              "      <td>Who was the Norse leader?</td>\n",
              "      <td>rollo</td>\n",
              "      <td>Rollo;Rollo;Rollo;Rollo</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56ddde6b9a695914005b962c</td>\n",
              "      <td>What century did the Normans first gain their ...</td>\n",
              "      <td>10th</td>\n",
              "      <td>10th century;the first half of the 10th centur...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b5c1a208-4dcb-4a92-96ce-d0ac89a716af')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b5c1a208-4dcb-4a92-96ce-d0ac89a716af button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b5c1a208-4dcb-4a92-96ce-d0ac89a716af');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-dc09341b-7f15-4291-b8bc-3a7f366424f8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dc09341b-7f15-4291-b8bc-3a7f366424f8')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-dc09341b-7f15-4291-b8bc-3a7f366424f8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df['f1'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O67NGSaajNbr",
        "outputId": "72e385f8-8979-4464-df15-a6e4b4d49d45"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7866849965752404"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_df['em'].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6fWqRwPjdEK",
        "outputId": "ff62a86b-4cd5-48ae-d16b-e20fe74cdd10"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.72"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Improving measurement functions through model thresholding"
      ],
      "metadata": {
        "id": "w0aK2KezjfbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we tokenize a question and context, and we pass it to the model, the output consists of two probabilities (logits). One is the start of the answer span, the other for the end of the answer span. <br><br>\n",
        "\n",
        "Every tokent that is passed to the model is assigned a logit, and tokens corresponding to the question itself. <br><br>\n",
        "\n",
        "Let's have a look at what this means, using a previous question (\"What happened 3.7 - 2 billion years ago?\"):"
      ],
      "metadata": {
        "id": "sFyfYSl_jnM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer.encode_plus(example.question_text, example.context_text, return_tensors = 'pt')\n",
        "output = model(**inputs)"
      ],
      "metadata": {
        "id": "C1kCEIubj2CO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking below, we can observe how large is the first position of the array, this is the [CLS] token position. This has a strong probability that this question has no answer, but we answered it anyway."
      ],
      "metadata": {
        "id": "tgoRG09wkS56"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_logits = output.start_logits\n",
        "end_logits = output.end_logits"
      ],
      "metadata": {
        "id": "ErCBy8aHbXGk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_logits"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuL9EdX2bb29",
        "outputId": "cac9da5e-e587-4583-8b90-07b57199675d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  5.1171,  -8.3404,  -9.2660,  -8.0987,  -9.1736,  -9.5905,  -9.6239,\n",
              "          -9.4974,  -9.7725,  -9.9778, -10.1417,  -9.6171,  -8.7191,  -3.9945,\n",
              "          -5.9036,  -6.7555,  -8.8160,  -7.9155,  -8.7225,  -9.7324,  -9.5704,\n",
              "         -10.2445,  -9.0550,  -7.5630,  -9.8712, -10.0757,  -7.6297,  -7.4481,\n",
              "          -6.5011,  -9.7018, -10.0513,  -9.4285,  -8.6840, -10.2489,  -9.9774,\n",
              "          -8.0076,  -7.7707,  -8.0321,  -6.8884,  -6.6425,  -6.5631,  -9.4518,\n",
              "          -8.6434,  -9.5573,  -9.8626,  -9.7323,  -7.0476,  -4.5778,  -6.4825,\n",
              "          -6.7371,  -6.9433,  -9.1568,  -6.9630,  -8.9800,  -6.9723,  -7.3322,\n",
              "          -5.2532,  -9.6134,  -9.4807, -10.0298,  -9.8842,  -8.8732,  -7.9342,\n",
              "          -8.2085,  -8.0398,  -7.8698,  -7.2027,  -9.6577,  -9.1055,  -9.7395,\n",
              "          -7.7438,  -9.5543,  -9.0663,  -9.2180,  -9.8046,  -9.6983,  -8.9082,\n",
              "          -6.9894,  -6.7306,  -7.6100,  -6.7132,  -8.6220,  -9.4562,  -8.3209,\n",
              "          -5.3854,  -6.1157,  -6.8288,  -8.7172,  -9.4399,  -8.5389,  -6.3029,\n",
              "          -6.8487,  -9.8548,  -7.2582,  -8.6202,  -9.2930,  -9.4980,  -6.5140,\n",
              "          -8.4708,  -6.9057,  -5.5324,  -7.3197,  -6.6870,  -7.6689,   0.6028,\n",
              "          -6.1649,  -1.1287,  -5.9593,  -5.8637,  -6.0899,  -5.6955,   1.3977,\n",
              "          -1.8839,  -1.7322,  -6.0377,  -2.1163,  -7.4047,  -6.2707,  -7.8654,\n",
              "          -4.9938,  -5.3458,  -9.1419,  -7.1310,  -8.7467,  -8.0603,  -8.0363,\n",
              "          -8.7677,  -8.3480,  -7.1321,  -1.9241,  -3.9768,  -8.1719,  -8.9737,\n",
              "          -7.6712,  -7.8334,  -7.1851,  -4.8624,  -3.9354,  -9.3268,  -9.7560,\n",
              "          -9.3310,  -9.5895,  -8.1781,  -7.8229,  -8.7144]],\n",
              "       grad_fn=<CloneBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model gets predictions by selecting the start and end tokens with the largest logits. It would be more sensible to choose any sensible start + end combinations as possible to answer the question."
      ],
      "metadata": {
        "id": "f_vAn4Febc7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These combinations can be scored independently and the one with the highest score would be considered the best answer."
      ],
      "metadata": {
        "id": "LKDrFDkWbrqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A possible (candidate) answer is scored as the sum of its start and end logits"
      ],
      "metadata": {
        "id": "wP9qmkH9bzbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating possible combinations"
      ],
      "metadata": {
        "id": "YN_ZpBrBb30i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by taking the n largest start and end logits. Any sensible combination can be considered an answer, however, some consistency checks must first be performed."
      ],
      "metadata": {
        "id": "9BnWrO4Fb8ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For instance:\n",
        "    \n",
        "    - End token must fall after the start token\n",
        "    - Candidate answers wherein the start or end tokens are associated with question tokens\n",
        "\n",
        "[CLS] is not removed from the answers as it can indicate null answer"
      ],
      "metadata": {
        "id": "Zp5Xk5t8cHOQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our start and end logit tensors to lists\n",
        "start_logits = to_list(start_logits)[0]\n",
        "end_logits = to_list(end_logits)[0]"
      ],
      "metadata": {
        "id": "nvXWppCfcJQ7"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort our start and end logits from the largest to the smallest, keeping track of the index\n",
        "start_idx_and_logit = sorted(enumerate(start_logits), key = lambda x: x[1], reverse = True)\n",
        "end_idx_and_logit = sorted(enumerate(end_logits), key = lambda x: x[1], reverse = True)"
      ],
      "metadata": {
        "id": "GEoHj9OqcYCm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select the top n (in this case, 5)\n",
        "print(start_idx_and_logit[: 5])\n",
        "print(end_idx_and_logit[: 5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5dcO9L2cvu7",
        "outputId": "7abe2244-85c3-4ea2-ae23-3f3dd725281f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, 5.117070198059082), (111, 1.3977439403533936), (104, 0.6027624607086182), (106, -1.1286697387695312), (113, -1.732158899307251)]\n",
            "[(0, 6.168291091918945), (119, 3.287285327911377), (109, 0.9794862270355225), (135, 0.3085411489009857), (116, -0.20684343576431274)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The null answer token (index 0) is in the top five of both the start and end logit lists."
      ],
      "metadata": {
        "id": "gBiIqxvHc4dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to eventually predict a text answer (or an empty string), we need to keep track of the indexes which will be use to pull the corresponding token ids later on. We'll also need to identify which indexes correspond to the question tokens, so we can ensure we don't alow a non-sensical prediction."
      ],
      "metadata": {
        "id": "6MAGkMrZdCAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_indexes = [idx for idx, logit in start_idx_and_logit[: 5]]\n",
        "end_indexes = [idx for idx, logit in end_idx_and_logit[: 5]]"
      ],
      "metadata": {
        "id": "ShUA8EPMdWVZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the token ids from a tensor to a list\n",
        "tokens = to_list(inputs['input_ids'])[0]"
      ],
      "metadata": {
        "id": "eNerZ6D3dgqt"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question tokens are defined as those between the CLS token (101, at position 0) and first SEP (102) token\n",
        "question_indexes = [i + 1 for i, token in enumerate(tokens[1: tokens.index(102)])]\n",
        "question_indexes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNn4gjL9dohV",
        "outputId": "95fb32b9-96a2-4e28-ec1d-d414221ef3ac"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# keep track of all preliminary predictions\n",
        "PrelimPrediction = collections.namedtuple(\n",
        "    \"PrelimPrediction\", [\"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n",
        ")"
      ],
      "metadata": {
        "id": "K-6wF7xOd8Sx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will generate a list of candidate predictions by looping through all combinations of the start and end token indexes, excluding non-sensical combinations."
      ],
      "metadata": {
        "id": "bZV--c0GeNNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prelim_preds = []\n",
        "\n",
        "for start_index in start_indexes:\n",
        "  for end_index in end_indexes:\n",
        "    # throw out invalid predictions\n",
        "    if start_index in question_indexes:\n",
        "      continue\n",
        "    if end_index in question_indexes:\n",
        "      continue\n",
        "    if end_index < start_index:\n",
        "      continue\n",
        "\n",
        "    prelim_preds.append(\n",
        "        PrelimPrediction(\n",
        "            start_index = start_index,\n",
        "            end_index = end_index,\n",
        "            start_logit = start_logits[start_index],\n",
        "            end_logit = end_logits[end_index]\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "futcP0OueZHS"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With a list of sensible candidate predictions, it's time to score them now."
      ],
      "metadata": {
        "id": "JULI2Ulfe7wV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a candidate answer, score = start_logit + end_logit. Below, we sort our candidate predictions by their score."
      ],
      "metadata": {
        "id": "_UdUavhve__y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sort preliminary predictions by their score\n",
        "prelim_preds = sorted(prelim_preds, key = lambda x: (x.start_logit + x.end_logit), reverse = True)\n",
        "print(prelim_preds[: 5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7A4WqtZpfJyl",
        "outputId": "8fbdb82d-1460-4f08-8354-6b992e43b0e0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PrelimPrediction(start_index=0, end_index=0, start_logit=5.117070198059082, end_logit=6.168291091918945), PrelimPrediction(start_index=0, end_index=119, start_logit=5.117070198059082, end_logit=3.287285327911377), PrelimPrediction(start_index=0, end_index=109, start_logit=5.117070198059082, end_logit=0.9794862270355225), PrelimPrediction(start_index=0, end_index=135, start_logit=5.117070198059082, end_logit=0.3085411489009857), PrelimPrediction(start_index=0, end_index=116, start_logit=5.117070198059082, end_logit=-0.20684343576431274)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to convert our preliminary predictions into actual text (or the empty string, if null). We will keep track of text predictions we've seen, because different token combinations can result in the same text prediction and we only want to keep the one with the highest score (we're looping in descending score order). Finally, we'll trim this list down to the best 5 predictions."
      ],
      "metadata": {
        "id": "qSKbrwUOfYre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keep track of all the best predictions\n",
        "BestPrediction = collections.namedtuple( # pylint: disable = invalid-name\n",
        "                                       \"BestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n",
        "                                        )"
      ],
      "metadata": {
        "id": "zj_Tzn4Pf4e1"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nbest = []\n",
        "seen_predictions = []\n",
        "\n",
        "for pred in prelim_preds:\n",
        "  # for now we only care about the top 5 best predictions\n",
        "  if len(nbest) >= 5:\n",
        "    break\n",
        "\n",
        "  # loop through the predictions according to their start index\n",
        "  if pred.start_index > 0: # non-null answers have start_index > 0\n",
        "    text = tokenizer.convert_tokens_to_string(\n",
        "        tokenizer.convert_ids_to_tokens(\n",
        "            tokens[pred.start_index: pred.end_index + 1]\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # clean the whitespace\n",
        "    text = text.strip()\n",
        "    text = \" \".join(text.split())\n",
        "\n",
        "    if text in seen_predictions:\n",
        "      continue\n",
        "\n",
        "    # flag this text as being seen - if we see it again, don't add it to the nbest list\n",
        "    seen_predictions.append(text)\n",
        "\n",
        "    # add this text prediction to a pruned list of the top 5 best predictions\n",
        "    nbest.append(BestPrediction(text = text, start_logit = pred.start_logit, end_logit = pred.end_logit))"
      ],
      "metadata": {
        "id": "hRonNle0gL-a"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nbest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UurAXerXhb_T",
        "outputId": "4e7c9372-84d8-4216-ee2a-413e5960ba1f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[BestPrediction(text='free oxygen began to outgas from the oceans', start_logit=1.3977439403533936, end_logit=3.287285327911377),\n",
              " BestPrediction(text='when such oxygen sinks became saturated , free oxygen began to outgas from the oceans', start_logit=0.6027624607086182, end_logit=3.287285327911377),\n",
              " BestPrediction(text='oxygen sinks became saturated , free oxygen began to outgas from the oceans', start_logit=-1.1286697387695312, end_logit=3.287285327911377),\n",
              " BestPrediction(text='free oxygen began to outgas from the oceans 3 – 2 . 7 billion years ago , reaching 10 % of its present level', start_logit=1.3977439403533936, end_logit=0.3085411489009857),\n",
              " BestPrediction(text='when such oxygen sinks became saturated', start_logit=0.6027624607086182, end_logit=0.9794862270355225)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have a neat list of the top 5 best predictions for this question, let's now also add the null answer."
      ],
      "metadata": {
        "id": "q8HfeMrthiEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# and don't forget -- include the null answer!\n",
        "nbest.append(BestPrediction(text = \"\", start_logit = start_logits[0], end_logit = end_logits[0]))\n",
        "nbest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1koiFyE0ncV",
        "outputId": "fcf826ba-590c-4786-e4b1-278604d9b6e1"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[BestPrediction(text='free oxygen began to outgas from the oceans', start_logit=1.3977439403533936, end_logit=3.287285327911377),\n",
              " BestPrediction(text='when such oxygen sinks became saturated , free oxygen began to outgas from the oceans', start_logit=0.6027624607086182, end_logit=3.287285327911377),\n",
              " BestPrediction(text='oxygen sinks became saturated , free oxygen began to outgas from the oceans', start_logit=-1.1286697387695312, end_logit=3.287285327911377),\n",
              " BestPrediction(text='free oxygen began to outgas from the oceans 3 – 2 . 7 billion years ago , reaching 10 % of its present level', start_logit=1.3977439403533936, end_logit=0.3085411489009857),\n",
              " BestPrediction(text='when such oxygen sinks became saturated', start_logit=0.6027624607086182, end_logit=0.9794862270355225),\n",
              " BestPrediction(text='', start_logit=5.117070198059082, end_logit=6.168291091918945)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The null answer is scored as the sum of the start_logit and end_logit associated with the [CLS] token."
      ],
      "metadata": {
        "id": "pWb7ltQz02aZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step is to compute the null score - more specifically, the difference between the null score and the best non-null score as shown below."
      ],
      "metadata": {
        "id": "9-afipUW0_g6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the null score as the sum of the [CLS] token logits\n",
        "score_null = start_logits[0] + end_logits[0]\n",
        "score_null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ2HACT01Hu4",
        "outputId": "5018864a-5092-4940-efb4-ea3cd25999da"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11.285361289978027"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nbest[0].start_logit + nbest[0].end_logit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrnRhWgD1RLa",
        "outputId": "d27d811b-4c3e-4f31-e243-01d68e592e8d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.6850292682647705"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compute the difference between the null score and tehbest non-null score\n",
        "score_diff = score_null - nbest[0].start_logit - nbest[0].end_logit\n",
        "score_diff"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiJXl2RG1YJm",
        "outputId": "aa7c4f7c-3647-4e86-ab8b-ea0fa906a9d1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.600332021713257"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQuAD Evaluation"
      ],
      "metadata": {
        "id": "eGKYB9ym1k3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model_name_or_path,\n",
        "             dataset,\n",
        "             output_dir,\n",
        "             per_gpu_eval_batch_size = 12,\n",
        "             n_gpu = 1,\n",
        "             model_type = BERT_MODEL_TYPE,\n",
        "             do_lower_case = DO_LOWER_CASE,\n",
        "             nbest_size = NBEST_SIZE,\n",
        "             max_answer_length = MAX_ANSWER_LENGTH,\n",
        "             null_score_diff_threshold = 0.0):\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast = False)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_name_or_path)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    eval_batch_size = per_gpu_eval_batch_size * max(1, n_gpu)\n",
        "\n",
        "    # Note that DistributedSampler samples randomly\n",
        "    eval_sampler = SequentialSampler(dataset)\n",
        "    eval_dataloader = DataLoader(dataset, sampler = eval_sampler, batch_size = eval_batch_size)\n",
        "\n",
        "    # multi-gpu evaluate\n",
        "    if n_gpu > 1 and not isinstance(model, torch.nn.DataParallel):\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "    all_results = []\n",
        "    start_time = timeit.default_timer()\n",
        "\n",
        "    for batch in tqdm(eval_dataloader, desc = \"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "            }\n",
        "\n",
        "            if model_type in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\", \"bart\", \"longformer\"]:\n",
        "                del inputs[\"token_type_ids\"]\n",
        "\n",
        "            feature_indices = batch[3]\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        for i, feature_index in enumerate(feature_indices):\n",
        "            eval_feature = features[feature_index.item()]\n",
        "            unique_id = int(eval_feature.unique_id)\n",
        "\n",
        "            output = [to_list(output[i]) for output in outputs.to_tuple()]\n",
        "\n",
        "            start_logits, end_logits = output\n",
        "            result = SquadResult(unique_id, start_logits, end_logits)\n",
        "\n",
        "            all_results.append(result)\n",
        "\n",
        "    evalTime = timeit.default_timer() - start_time\n",
        "    print(f\"Evaluation done in total {evalTime} seconds ({evalTime / len(dataset)} seconds per example)\")\n",
        "\n",
        "    # compute predictions\n",
        "    os.makedirs(output_dir, exist_ok = True)\n",
        "\n",
        "    output_prediction_file = os.path.join(output_dir, \"predictions.json\")\n",
        "    output_nbest_file = os.path.join(output_dir, \"nbest_predictions.json\")\n",
        "\n",
        "    output_null_log_odds_file = os.path.join(output_dir, \"null_odds.json\")\n",
        "\n",
        "    predictions = compute_predictions_logits(\n",
        "        examples,\n",
        "        features,\n",
        "        all_results,\n",
        "        nbest_size,\n",
        "        max_answer_length,\n",
        "        do_lower_case,\n",
        "        output_prediction_file,\n",
        "        output_nbest_file,\n",
        "        output_null_log_odds_file,\n",
        "        False,\n",
        "        True,\n",
        "        null_score_diff_threshold,\n",
        "        tokenizer,\n",
        "    )\n",
        "\n",
        "    # compute the F1 and EM scores\n",
        "    results = squad_evaluate(examples, predictions)\n",
        "\n",
        "    results.update({\"eval_time\": evalTime, \"prediction_time\": evalTime / len(dataset)})\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "SY0Xwyfi1hFU"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = evaluate(BERT_MODEL_HF_PATH, dataset, BERT_OUTPUT_DIR)\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-6c1B96QTE",
        "outputId": "f3fde5be-f9c2-438a-af54-7202183c0842"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at twmkn9/bert-base-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Evaluating: 100%|██████████| 1020/1020 [5:08:38<00:00, 18.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation done in total 18518.158740810002 seconds (1.5139109500335188 seconds per example)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('exact', 72.3658721468879),\n",
              "             ('f1', 75.83107045305708),\n",
              "             ('total', 11873),\n",
              "             ('HasAns_exact', 72.80701754385964),\n",
              "             ('HasAns_f1', 79.74735146578041),\n",
              "             ('HasAns_total', 5928),\n",
              "             ('NoAns_exact', 71.9259882253995),\n",
              "             ('NoAns_f1', 71.9259882253995),\n",
              "             ('NoAns_total', 5945),\n",
              "             ('best_exact', 72.3658721468879),\n",
              "             ('best_exact_thresh', 0.0),\n",
              "             ('best_f1', 75.8310704530571),\n",
              "             ('best_f1_thresh', 0.0),\n",
              "             ('eval_time', 18518.158740810002),\n",
              "             ('prediction_time', 1.5139109500335188)])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first three blocks of the results output are pretty straightforward. EM and F1 scores are reported over\n",
        "\n",
        "\n",
        "*   the full dev set,\n",
        "*   the set of positive examples,\n",
        "*   the set of negative examples.\n",
        "\n",
        "This can provide some insight into whether a model is performing adequately on both answer and no-answer questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "mr4qA7rK6a8W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per below results, we see that two sections are zero. They depend on setting a threshold for the model so it knows when to prefer a null answer over an actual answer."
      ],
      "metadata": {
        "id": "uQLqLuelGRV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In other words, we should predict a null answer for a given example if that example's score difference is above a certain threshold. What should be that threshold? How should we compute it? They give us a recipe: select the threshold that maximizes F1."
      ],
      "metadata": {
        "id": "Mh9vXQMzGcNZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will leverage the files created by the evaluate function above to retrieve the optimal threshold."
      ],
      "metadata": {
        "id": "J_NdOYRFGtWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "id": "hpv-xlDIG0m3",
        "outputId": "2cbfb99a-8667-485d-edcd-3a6b9133f8dc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-b7133701d76c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ShYLaenHE4d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}